{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchcam -y -q"
      ],
      "metadata": {
        "id": "ClWrqP_lRSJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchcam==0.3.0 numpy pandas matplotlib earthengine-api geopandas -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbCCeuAURUIz",
        "outputId": "538cd136-5b70-4555-b98c-05a349d819c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "q2iOORr9MWZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ee.Authenticate()\n",
        "ee.Initialize(project='ndvi-imagery')"
      ],
      "metadata": {
        "id": "OUduEPPPPOl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SensorDataGenerator:\n",
        "    def __init__(self, seed=42):\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "    def generate(self, label):\n",
        "        if label == \"healthy\":\n",
        "            return [self.rng.uniform(30, 40), self.rng.uniform(18, 28),\n",
        "                    self.rng.uniform(60, 80), self.rng.uniform(6.0, 7.0)]\n",
        "        elif label == \"unhealthy\":\n",
        "            return [self.rng.uniform(20, 30), self.rng.uniform(28, 35),\n",
        "                    self.rng.uniform(50, 70), self.rng.uniform(5.5, 6.0)]\n",
        "        else:  # drought\n",
        "            return [self.rng.uniform(10, 20), self.rng.uniform(32, 40),\n",
        "                    self.rng.uniform(40, 60), self.rng.uniform(5.0, 5.5)]"
      ],
      "metadata": {
        "id": "m7Ogzr7cMcKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NDVI Download from GEE\n",
        "def get_ndvi_image(coords, date_start, date_end, scale=64):\n",
        "    region = ee.Geometry.Polygon(coords)\n",
        "    s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
        "           .filterBounds(region) \\\n",
        "           .filterDate(date_start, date_end) \\\n",
        "           .median()\n",
        "    ndvi = s2.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
        "    # Consider if sampleRectangle is the best approach for getting a 64x64 array\n",
        "    # depending on the size of 'region'. If region is large, sampleRectangle might\n",
        "    # sample points within the region rather than providing a structured grid.\n",
        "    # For a fixed 64x64 image, you might need to use .getRegion() and reshape or\n",
        "    # .reduceRegion() with a grid. However, for this error fix, we'll keep the original logic.\n",
        "    arr = ndvi.sampleRectangle(region=region).get('NDVI').getInfo()\n",
        "    return np.array(arr).astype(np.float32)  # Should be 64x64 if region is appropriate size"
      ],
      "metadata": {
        "id": "e3ZwGmxPMeKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class\n",
        "class CropDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000):\n",
        "        self.sensor_gen = SensorDataGenerator()\n",
        "        self.labels = [\"healthy\"]*600 + [\"unhealthy\"]*300 + [\"drought-affected\"]*100\n",
        "        self.ndvi_samples = []\n",
        "\n",
        "        # Simulate NDVI images (in practice, use GEE)\n",
        "        for label in self.labels:\n",
        "            if label == \"healthy\":\n",
        "                self.ndvi_samples.append(np.random.uniform(0.6, 0.9, (64, 64)))\n",
        "            elif label == \"unhealthy\":\n",
        "                self.ndvi_samples.append(np.random.uniform(0.3, 0.6, (64, 64)))\n",
        "            else: # drought-affected\n",
        "                self.ndvi_samples.append(np.random.uniform(0.0, 0.3, (64, 64)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ndvi = self.ndvi_samples[idx]\n",
        "        label = self.labels[idx]\n",
        "        sensors = self.sensor_gen.generate(label)\n",
        "\n",
        "        # Ensure labels are correctly mapped to indices\n",
        "        label_map = {\"healthy\": 0, \"unhealthy\": 1, \"drought-affected\": 2}\n",
        "        label_idx = label_map[label]\n",
        "\n",
        "        return (\n",
        "            torch.tensor(ndvi, dtype=torch.float32).unsqueeze(0),  # Add channel dim [1, 64, 64] and explicitly set dtype\n",
        "            torch.tensor(sensors, dtype=torch.float32), # Ensure sensor data is float\n",
        "            torch.tensor(label_idx)\n",
        "        )"
      ],
      "metadata": {
        "id": "d3w1VbWRMf8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Architecture\n",
        "class FusionLiteCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Input image size is 64x64\n",
        "        # Conv1: (1, 64, 64) -> (32, 64, 64) after padding\n",
        "        # MaxPool1: (32, 64, 64) -> (32, 32, 32)\n",
        "        # Conv2: (32, 32, 32) -> (64, 32, 32) after padding\n",
        "        # MaxPool2: (64, 32, 32) -> (64, 16, 16)\n",
        "        # Flatten: (64, 16, 16) -> 64 * 16 * 16 = 16384 features\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        # MLP for sensor data (4 features)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(4, 16), nn.ReLU(),\n",
        "            nn.Linear(16, 32) # Outputting 32 features\n",
        "        )\n",
        "        # Classifier combining CNN output (16384) and MLP output (32)\n",
        "        # Total features = 16384 + 32 = 16416\n",
        "        self.classifier = nn.Linear(64*16*16 + 32, 3) # Outputting 3 classes\n",
        "\n",
        "    def forward(self, img, sensors):\n",
        "        img_feat = self.cnn(img)\n",
        "        sensor_feat = self.mlp(sensors)\n",
        "        return self.classifier(torch.cat([img_feat, sensor_feat], dim=1))\n"
      ],
      "metadata": {
        "id": "nMgYH_ojMiWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "def train():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = FusionLiteCNN().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # num_samples=1000 is default, explicitly setting here for clarity\n",
        "    dataset = CropDataset(num_samples=1000)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        for img, sensors, label in loader:\n",
        "            img, sensors, label = img.to(device), sensors.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(img, sensors)\n",
        "            loss = criterion(outputs, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * img.size(0) # Accumulate loss weighted by batch size\n",
        "        epoch_loss = running_loss / len(dataset) # Calculate average loss per epoch\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), 'fusion_model.pth')\n",
        "    print(\"Model saved!\")\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d0obv4qMjvm",
        "outputId": "a2aab0df-a70e-4a2a-934f-da0e1b301c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2165\n",
            "Epoch 2, Loss: 0.0000\n",
            "Epoch 3, Loss: 0.0000\n",
            "Epoch 4, Loss: 0.0000\n",
            "Epoch 5, Loss: 0.0000\n",
            "Epoch 6, Loss: 0.0000\n",
            "Epoch 7, Loss: 0.0000\n",
            "Epoch 8, Loss: 0.0000\n",
            "Epoch 9, Loss: 0.0000\n",
            "Epoch 10, Loss: 0.0000\n",
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp fusion_model.pth /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-kBWUzEMmmW",
        "outputId": "2b63d2a6-ee36-4d06-8f7c-1065dfd1164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lW_L8tMGMoxc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}